{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d18642c1bcaec05236c28de039e7ce2f6512f6d3"
   },
   "source": [
    "## CNN similar to VGG16 with random Weight Initilizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "834a681c1d797530f254b850145abb4378111a99"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import timeit\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af2dc7552bd0c835fdb741396c052757d189c8b8"
   },
   "outputs": [],
   "source": [
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, datafolder, datatype='train', transform = transforms.Compose([transforms.ToTensor()]), labels_dict={}):\n",
    "        self.datafolder = datafolder\n",
    "        self.datatype = datatype\n",
    "        self.image_files_list = [s for s in os.listdir(datafolder)]\n",
    "        self.transform = transform\n",
    "        self.labels_dict = labels_dict\n",
    "        if self.datatype == 'train':\n",
    "            self.labels = [labels_dict[i.split('.')[0]] for i in self.image_files_list]\n",
    "        else:\n",
    "            self.labels = [0 for _ in range(len(self.image_files_list))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.datafolder, self.image_files_list[idx])\n",
    "        image = Image.open(img_name)\n",
    "        image = self.transform(image)\n",
    "        img_name_short = self.image_files_list[idx].split('.')[0]\n",
    "\n",
    "        if self.datatype == 'train':\n",
    "            label = self.labels_dict[img_name_short]\n",
    "        else:\n",
    "            label = 0\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "IMAGE_NOT_FOUND_COUNTER = 0\n",
    "\n",
    "labels = pd.read_csv('../input/train_labels.csv')\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    #transforms.CenterCrop(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "data_transforms_test = transforms.Compose([\n",
    "    #transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "tr, val = train_test_split(labels.label, stratify=labels.label, test_size=0.1)\n",
    "print(\"number of training data: \",len(tr))\n",
    "print(\"number of testing  data: \",len(val))\n",
    "# dictionary with labels and ids of train data\n",
    "img_class_dict = {k:v for k, v in zip(labels.id, labels.label)}\n",
    "\n",
    "train_sampler = SubsetRandomSampler(list(tr.index))\n",
    "valid_sampler = SubsetRandomSampler(list(val.index))\n",
    "batch_size = 256\n",
    "num_workers = 0\n",
    "\n",
    "dataset = CancerDataset(datafolder='../input/train/', datatype='train', transform=data_transforms, labels_dict=img_class_dict)\n",
    "test_set = CancerDataset(datafolder='../input/test/', datatype='test', transform=data_transforms_test)\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90bae2c4a31beaf2945d2f54c9f092155bfed1a9"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9112b0d404cc38ead7693b7ee6c9cb5cf792d4b4"
   },
   "outputs": [],
   "source": [
    "avg_loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "def train(model, train_loader ,loss_fn, optimizer, num_epochs = 1):\n",
    "    total_loss =0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(train_loader):\n",
    "            x_var = Variable(x.type(gpu_dtype))\n",
    "            y_var = Variable(y.type(gpu_dtype).long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            loss = loss_fn(scores, y_var)\n",
    "            total_loss += loss.data\n",
    "            \n",
    "            if (t + 1) % print_every == 0:\n",
    "                avg_loss = total_loss/print_every\n",
    "                print('t = %d, avg_loss = %.4f' % (t + 1, avg_loss) )\n",
    "                avg_loss_list.append(avg_loss)\n",
    "                total_loss = 0\n",
    "                \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        acc = check_accuracy(fixed_model_gpu, valid_loader)\n",
    "        print('acc = %f' %(acc))\n",
    "            \n",
    "def check_accuracy(model, loader):\n",
    "    print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(gpu_dtype))\n",
    "\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    acc_list.append(acc)\n",
    "    return acc\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cdcb7f6c9adffc8b1b2862773997e7f32726f5fc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "print_every = 20\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "out_1 = 32\n",
    "out_2 = 64\n",
    "out_3 = 128\n",
    "out_4 = 256\n",
    "\n",
    "k_size_1 = 3\n",
    "padding_1 = 1\n",
    "\n",
    "\n",
    "num_epochs = 16\n",
    "\n",
    "\n",
    "# the following model is similar to VGG16 model\n",
    "fixed_model_base = nn.Sequential( # You fill this in!\n",
    "                nn.Conv2d(3, out_1, padding= padding_1, kernel_size=k_size_1, stride=1), # out_1-k_size_1+1 = 26\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(out_1),\n",
    "                nn.Conv2d(out_1 , out_1, padding= padding_1, kernel_size=k_size_1, stride=1), #26 - 4 + 1 = 23\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(out_1),\n",
    "                nn.Conv2d(out_1 , out_1, padding= padding_1, kernel_size=k_size_1, stride=1), # 23 -3 = 20\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(out_1),\n",
    "    \n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "    \n",
    "                nn.Conv2d(out_1 , out_2, padding= padding_1, kernel_size=k_size_1, stride=1), # 20 -3 = 17\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(out_2),\n",
    "                nn.Conv2d(out_2 , out_2, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
    "                nn.ReLU(inplace=True), \n",
    "                nn.BatchNorm2d(out_2),\n",
    "                nn.Conv2d(out_2 , out_2, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(out_2),\n",
    "    \n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "    \n",
    "                nn.Conv2d(out_2 , out_3, padding= padding_1, kernel_size=k_size_1, stride=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(out_3),\n",
    "                nn.Conv2d(out_3 , out_3, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(out_3),\n",
    "                nn.Conv2d(out_3 , out_3, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(out_3),\n",
    "    \n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "    \n",
    "                nn.Conv2d(out_3 , out_4, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(out_4),\n",
    "                nn.Conv2d(out_4 , out_4, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(out_4),\n",
    "                nn.Conv2d(out_4 , out_4, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(out_4),\n",
    "    \n",
    "                #nn.Conv2d(out_11 , out_12, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
    "                #nn.ReLU(inplace=True),\n",
    "                #nn.BatchNorm2d(out_12),\n",
    "    \n",
    "                nn.MaxPool2d(2, stride=2), #17/2 = 7\n",
    "                Flatten(),\n",
    "                \n",
    "                nn.Linear(9216,512 ), # affine layer\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(512,10), # affine layer\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(10,2), # affine layer\n",
    "            )\n",
    "fixed_model_gpu = fixed_model_base.type(gpu_dtype)\n",
    "print(fixed_model_gpu)\n",
    "loss_fn = nn.modules.loss.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(fixed_model_gpu.parameters(), lr = 1e-3)\n",
    "\n",
    "train(fixed_model_gpu, train_loader ,loss_fn, optimizer, num_epochs=num_epochs)\n",
    "check_accuracy(fixed_model_gpu, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b2f26fc0e0f59be1eba5774d5d68c1b6f581ef6"
   },
   "outputs": [],
   "source": [
    "print(avg_loss_list,acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn for plotting and styling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6849f23812e7deb06c2500395f80015115bfce2f"
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (15, 7))\n",
    "\n",
    "ax[0].plot([print_every*batch_size*(i+1)/len(tr) for i in range((len(avg_loss_list)))],avg_loss_list, label='Training loss');\n",
    "test_results= [i+1 for i in range((len(acc_list)) - 1)]\n",
    "ax[0].legend()\n",
    "ax[1].plot(test_results,acc_list[:-1], label='Training accuracy', color='y')\n",
    "ax[1].set(ylim=(0, 1))\n",
    "\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6088c735b19731d3c5cfdd28811ceca346cabc66"
   },
   "outputs": [],
   "source": [
    "fixed_model_gpu.eval()\n",
    "preds = []\n",
    "for batch_i, (data, target) in enumerate(test_loader):\n",
    "    data, target = data.cuda(), target.cuda()\n",
    "    output = fixed_model_gpu(data)\n",
    "\n",
    "    pr = output[:,1].detach().cpu().numpy()\n",
    "    for i in pr:\n",
    "        preds.append(i)\n",
    "        \n",
    "test_preds = pd.DataFrame({'imgs': test_set.image_files_list, 'preds': preds})\n",
    "\n",
    "test_preds['imgs'] = test_preds['imgs'].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "data_to_submit = pd.read_csv('../input/sample_submission.csv')\n",
    "data_to_submit = pd.merge(data_to_submit, test_preds, left_on='id', right_on='imgs')\n",
    "data_to_submit = data_to_submit[['id', 'preds']]\n",
    "data_to_submit.columns = ['id', 'label']\n",
    "data_to_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4011f2fe1ba4c87cd64cc792ee37fd7977b65432"
   },
   "outputs": [],
   "source": [
    "data_to_submit.to_csv('csv_to_submit.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a4baf77ec397302c7c35af473595f74b6cf240c"
   },
   "source": [
    "#citation\n",
    "* data parsing and code for submittion are taken from: https://www.kaggle.com/artgor/simple-eda-and-model-in-pytorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
